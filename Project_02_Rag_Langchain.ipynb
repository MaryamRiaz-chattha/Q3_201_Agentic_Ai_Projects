{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4/vNNKZoFbO2vZdHrfSCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaryamRiaz-chattha/Q3_201_Agentic_Ai_Projects/blob/main/Project_02_Rag_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Project_02_Rag_Langchain***"
      ],
      "metadata": {
        "id": "Y-gW9_Pw1l11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submitted by:Maryam Riaz"
      ],
      "metadata": {
        "id": "3ZiE3UPO1zsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**\n",
        "- Installing LangChain, PyPDF, Google GenAI, and Chroma for AI and PDF handling.\n",
        "- Upgrades Google authentication libraries for secure API access.\n",
        "- Uses the `-q` flag to install quietly and `--upgrade` to get the latest versions."
      ],
      "metadata": {
        "id": "IZcxjkMFyC1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVOXQwfJq8Tl",
        "outputId": "b45acc2e-18d6-4127-fec0-4ee25e08eac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.37.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install langchain_community pypdf -q langchain_google_genai  langchain_chroma\n",
        "\n",
        "\n",
        "!pip install  --upgrade -q google-auth google-auth-oauthlib google-auth-httplib2  google-cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:**\n",
        "\n",
        "1. **Import the PDF loader**:  \n",
        "   Import the necessary class `PyPDFLoader` from LangChain to handle PDF files.\n",
        "\n",
        "2. **Load the PDF file**:  \n",
        "   Use the `PyPDFLoader` to load the PDF document from a specified path (`final_Report.pdf`).\n",
        "\n",
        "3. **Process the content**:  \n",
        "   Use the `.load()` method to extract the content of the PDF into a variable.\n",
        "\n",
        "4. **Check the length**:  \n",
        "   Use `len(data)` to determine how many items (like pages) were loaded.\n",
        "\n",
        "5. **View the PDF**:  \n",
        "   You can view the full PDF document by following the [Google Drive link](https://drive.google.com/file/d/1MtfaJu0a5r1W2RpmjEnclZADyRiAYt1h/view?usp=sharing).."
      ],
      "metadata": {
        "id": "1hygt5U-yTDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/final_Report.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2egISmGrLry",
        "outputId": "3859c1d5-a14c-4b36-dbd0-7b71bbf7efe1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:**\n",
        "\n",
        "1. **Text Splitting**:\n",
        "   - **`from langchain.text_splitter import RecursiveCharacterTextSplitter`**: Imports the `RecursiveCharacterTextSplitter` class, which splits long text into smaller chunks based on a specified size.\n",
        "   - **`text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)`**: Creates an instance of `RecursiveCharacterTextSplitter` with a chunk size of 1000 characters.\n",
        "   - **`docs = text_splitter.split_documents(data)`**: Splits the `data` (the loaded PDF content) into smaller chunks and stores the result in `docs`.\n",
        "\n",
        "2. **Print Number of Documents**:\n",
        "   - **`print(\"total number of documents : \" , len(docs))`**: Prints the total number of chunks (documents) generated after splitting the text.\n",
        "\n",
        "3. **Google API Key Retrieval**:\n",
        "   - **`from google.colab import userdata`**: Imports the `userdata` module to access stored user data in Colab.\n",
        "   - **`GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY_1')`**: Retrieves the Google API key stored under the name `GOOGLE_API_KEY_1` and stores it in the `GOOGLE_API_KEY` variable.\n",
        "\n",
        "This code splits the text from the PDF into smaller chunks and retrieves the Google API key from the Colab environment."
      ],
      "metadata": {
        "id": "oPI2_kFRyvQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "\n",
        "print(\"total number of documents : \" ,len(docs))\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY_1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1SM8Te-rcB0",
        "outputId": "54cab7e2-6ddb-43f8-80fa-e961a9859038"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of documents :  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:**\n",
        "1. **Set Google Cloud Credentials**: Sets the environment variable for Google Cloud authentication using a JSON credentials file.\n",
        "2. **Import Google AI Embeddings**: Imports a class to integrate Google Generative AI for creating embeddings.\n",
        "3. **Import Chroma**: Imports Chroma for storing and querying embeddings efficiently."
      ],
      "metadata": {
        "id": "ICflxJ1Uy_lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/rag.json\"\n",
        "\n",
        "\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "lj2vqQ8CrqWr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:**\n",
        "1. **Create Embedding Model**: Initializes a Google Generative AI model for embeddings using the API key.\n",
        "2. **Create Vector Store**: Stores document embeddings in Chroma for efficient search.\n",
        "3. **Create Retriever**: Configures a retriever to search for the top 10 most similar documents."
      ],
      "metadata": {
        "id": "qvGvdgvkzf1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\" , google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n"
      ],
      "metadata": {
        "id": "qPdKx_FXryRA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:**\n",
        "1. **Import Chat Model**: Imports `ChatGoogleGenerativeAI` for Google’s chat-based AI.\n",
        "2. **Create Model Instance**: Initializes the chat model (`gemini-2.0-flash-exp`) with specified temperature (0.3) and token limit (1000)."
      ],
      "metadata": {
        "id": "3nHXrUHfzye1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-exp\" , temperature = 0.3 , max_tokens=1000)"
      ],
      "metadata": {
        "id": "n9EHPNA-r-sA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:**\n",
        "1. **Import Functions**: Imports functions for creating retrieval chains and prompt templates.\n",
        "2. **Define System Prompt**: Sets up a system message to guide concise Q&A responses.\n",
        "3. **Create Chat Prompt Template**: Creates a prompt template with system and user messages for handling queries."
      ],
      "metadata": {
        "id": "c8ehV_SM0BDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "HbYMcTjBwl_6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8:**\n",
        "\n",
        "1. **Create Document Chain**: **`question_answer_chain = create_stuff_documents_chain(llm, prompt)`** creates a chain that combines the document retrieval process with the generative AI model (`llm`) and the defined prompt.\n",
        "2. **Create Retrieval Chain**: **`rag_chain = create_retrieval_chain(retriever, question_answer_chain)`** sets up a retrieval-augmented generation (RAG) chain, linking the retriever and document chain for answering questions."
      ],
      "metadata": {
        "id": "E_W1cGPJ0Ldh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        ""
      ],
      "metadata": {
        "id": "qbcCwxd7wr-1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9:**\n",
        "1. **Invoke RAG Chain**: Queries the RAG chain with an unrelated question.\n",
        "2. **Print Answer**: Outputs the generated answer from the model."
      ],
      "metadata": {
        "id": "QKBS_TKp0eoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\" : \"what is LLM\"})\n",
        "print(response[\"answer\"])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA0Ljhueww3c",
        "outputId": "2d38c1cb-b092-46ed-c74b-8f4686d05526"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but the provided documents do not contain information about LLMs. Therefore, I cannot answer your question.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10:**\n",
        "1. **Invoke RAG Chain**: Queries the RAG chain with the related to pdf question .\n",
        "2. **Print Answer**: Outputs the generated answer from the model."
      ],
      "metadata": {
        "id": "DLM8AHoa0zAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = rag_chain.invoke({\"input\" : \"what is web based online voting system.Which GUI is used . Who Developed this. Also tell who was the supervisor?\"})\n",
        "\n",
        "answer = response[\"answer\"]\n",
        "\n",
        "# Formatting the full answer without separating the questions\n",
        "formatted_answer = f\"Here is the detailed information:\\n\\n{answer.strip()}\"\n",
        "\n",
        "# Print the formatted answer\n",
        "print(formatted_answer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlhILYi31YyA",
        "outputId": "eaafe338-3ee7-4483-d1f6-24c249d7eaf5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the detailed information:\n",
            "\n",
            "The Web-Based Online Voting System is a platform that allows voters to cast their votes online through a website or any internet-connected device. The system's main interface includes a voter registration section. Maryam Riaz developed this system as part of her Mcs at Virtual University of Pakistan, and Kainat Malik was the project supervisor.\n"
          ]
        }
      ]
    }
  ]
}